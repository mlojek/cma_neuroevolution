{
    "optimization_type": "gradient",
    "optimization_method": "adam",
    "epochs": 50,
    "learning_rate": 0.01,
    "dataset": {
        "batch_size": 16,
        "test_size": 0.2
    },
    "logging": {
        "use_wandb": false,
        "log_interval": 5
    }
}